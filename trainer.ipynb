{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9cdddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SETUP\n",
    "!pip install -q transformers datasets accelerate sentencepiece torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "\n",
    "# 2. LOGIN\n",
    "notebook_login()\n",
    "\n",
    "# 3. CONFIGURATION (Using the robust Base model)\n",
    "MODEL_CHECKPOINT = \"google-t5/t5-base\"\n",
    "OUTPUT_DIR = \"t5-base-sql-custom\"\n",
    "HUB_MODEL_ID = \"hmyunis/t5-base-sql-custom\"\n",
    "\n",
    "# 4. LOAD & PREPARE\n",
    "dataset = load_dataset('json', data_files='train.json', split='train')\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # T5 prefers \"translate English to SQL: ...\" prefix\n",
    "    inputs = [f\"translate English to SQL: {q} </s> {c}\" for q, c in zip(examples['question'], examples['context'])]\n",
    "    targets = examples['answer']\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Replace padding token id with -100 so we don't train on padding\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "# Split 90% train, 10% test\n",
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "\n",
    "# 5. TRAIN (Carefully tuned hyperparameters)\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-4, # Slightly higher for T5-base to kickstart it\n",
    "    per_device_train_batch_size=4, # Smaller batch for stability\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=12,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HUB_MODEL_ID,\n",
    "    fp16=True,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "trainer.train()\n",
    "\n",
    "# 6. VALIDATE BEFORE PUSHING (Sanity Check)\n",
    "print(\"Running Sanity Check...\")\n",
    "input_text = \"translate English to SQL: Show me all customers </s> api_customer: id, name\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(input_ids, max_length=100)\n",
    "print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# 7. PUSH\n",
    "trainer.push_to_hub()\n",
    "print(f\"Success! Model pushed to: https://huggingface.co/{HUB_MODEL_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api-3mS3PfHu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
